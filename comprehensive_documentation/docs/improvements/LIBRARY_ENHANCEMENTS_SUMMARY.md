# HEP-ML-Templates Library Enhancements

## üöÄ Summary of Improvements

Two major enhancements have been successfully implemented to make the HEP-ML-Templates library more user-friendly and complete:

1. **Embedded Extras Manager** - Integrated package management directly into the CLI
2. **Data Splitting Functionality** - Complete train/test/validation splitting with HEP-specific features

---

## 1. üì¶ Embedded Extras Manager

### What Was Added
- **Integrated CLI Commands**: Extras management is now built into the main `mlpipe` command
- **Standalone Manager**: Available as separate `mlpipe-manager` command for dedicated use
- **Complete Package Management**: List, validate, preview, and install functionality

### Available Commands

#### Via Main CLI (`mlpipe`):
```bash
# List all available extras with categorization
mlpipe list-extras

# Validate extras configuration
mlpipe validate-extras

# Show detailed information about specific extra
mlpipe extra-details model-xgb

# Preview what would be installed (useful for planning)
mlpipe preview-install model-xgb preprocessing

# Install extras to local directory
mlpipe install-local model-xgb ./my-project --target-dir
```

#### Via Standalone Manager (`mlpipe-manager`):
```bash
# Same functionality with shorter commands
mlpipe-manager list
mlpipe-manager validate  
mlpipe-manager details model-xgb
mlpipe-manager preview model-xgb preprocessing
mlpipe-manager install model-xgb ./my-project
```

### Enhanced Features

#### **Categorized Listings**
Extras are now organized into logical categories:
- üéØ **Complete Pipelines**: End-to-end ML workflows
- üß† **Individual Models**: Single algorithm implementations  
- ‚ö° **Algorithm Combos**: Model + preprocessing bundles
- üèóÔ∏è **Component Categories**: Preprocessing, evaluation, etc.
- üìä **Data Sources**: Dataset loaders and examples
- üåü **Special**: Comprehensive packages like 'all'

#### **Validation System**  
- Automatic verification that all referenced files exist
- Clear error reporting for missing components
- Statistics on package coverage and composition

#### **Installation Preview**
- See exactly what will be installed before committing
- Understand dependencies between extras
- Plan storage requirements

### Files Modified/Added
- `src/mlpipe/cli/manager.py` (new) - Standalone manager implementation
- `src/mlpipe/cli/main.py` - Added extras management commands
- `pyproject.toml` - Added `mlpipe-manager` entry point
- `src/mlpipe/cli/local_install.py` - Enhanced with validation functions

---

## 2. üî¨ Data Splitting Functionality  

### What Was Added
- **Complete Data Splitting Block**: Train/test/validation splits with HEP-specific features
- **Multiple Split Strategies**: Random, stratified, and time series splits
- **Ready-to-Use Configurations**: Pre-configured YAML files for common scenarios
- **Seamless Integration**: Works with existing pipeline architecture

### Key Features

#### **Flexible Split Configurations**
```python
# Basic train/test (80/20)
splits = split_data(X, y, train_size=0.8, test_size=0.2)

# Train/validation/test (70/15/15) with stratification  
splits = split_data(X, y, train_size=0.7, val_size=0.15, test_size=0.15, stratify=True)

# Time series split (preserves temporal order)
splits = split_data(X, y, train_size=0.7, val_size=0.15, test_size=0.15, time_series=True)
```

#### **HEP-Specific Features**
- **Stratified Sampling**: Maintains class balance for signal/background classification
- **Time Series Support**: Preserves temporal order for time-dependent analyses
- **Physics-Aware**: Handles typical HEP dataset characteristics
- **Reproducibility**: Built-in random seed control

#### **Three Usage Methods**

1. **Convenience Function** (Quick & Simple):
   ```python
   from mlpipe.blocks.preprocessing.data_split import split_data
   splits = split_data(X, y, train_size=0.8, test_size=0.2, stratify=True)
   X_train, y_train = splits['train']
   X_test, y_test = splits['test']
   ```

2. **Class-Based** (Full Control):
   ```python
   from mlpipe.blocks.preprocessing.data_split import DataSplitter
   
   config = {
       'train_size': 0.7, 'val_size': 0.15, 'test_size': 0.15,
       'stratify': True, 'random_state': 42
   }
   splitter = DataSplitter(config)
   splits = splitter.fit_transform(X, y)
   ```

3. **Pipeline Integration** (Config-Driven):
   ```bash
   # Use pre-configured splits in pipelines
   mlpipe run --overrides preprocessing=train_val_test_split
   ```

### Pre-Configured Options

#### **train_test_split.yaml** - Basic 80/20 Split
```yaml
train_size: 0.8
test_size: 0.2
stratify: false
shuffle: true
random_state: 42
```

#### **train_val_test_split.yaml** - 70/15/15 with Validation
```yaml
train_size: 0.7
val_size: 0.15
test_size: 0.15
stratify: true
shuffle: true
random_state: 42
```

#### **time_series_split.yaml** - Temporal Preservation
```yaml
train_size: 0.7
val_size: 0.15  
test_size: 0.15
time_series: true
shuffle: false
```

### New Extras Available

- **`data-split`**: Basic train/test splitting
- **`data-split-validation`**: Train/validation/test splitting
- **`data-split-timeseries`**: Time series splitting
- **Updated `preprocessing`**: Includes all splitting options
- **Updated `all`**: Complete package with data splitting

### Installation

```bash
# Get basic data splitting
mlpipe install-local data-split ./my-project

# Get validation set splitting  
mlpipe install-local data-split-validation ./my-project

# Get time series splitting
mlpipe install-local data-split-timeseries ./my-project

# Get complete preprocessing (includes all splitting)
mlpipe install-local preprocessing ./my-project
```

### Files Added
- `src/mlpipe/blocks/preprocessing/data_split.py` - Main implementation
- `configs/preprocessing/train_test_split.yaml` - Basic config
- `configs/preprocessing/train_val_test_split.yaml` - Validation config  
- `configs/preprocessing/time_series_split.yaml` - Time series config
- Updated `src/mlpipe/blocks/preprocessing/__init__.py` - Registration
- Updated extras mappings in `local_install.py`

---

## üéØ Benefits for Users

### Simplified Workflow
1. **Discover**: `mlpipe list-extras` shows all available components
2. **Plan**: `mlpipe preview-install` shows exactly what you'll get
3. **Install**: `mlpipe install-local` sets up your local project  
4. **Split**: Built-in data splitting with physics-aware defaults
5. **Train**: Use standard ML workflows with properly split data

### Production Ready
- ‚úÖ **Validated**: All file references confirmed to exist
- ‚úÖ **Modular**: Mix and match components as needed
- ‚úÖ **Configurable**: Extensive customization options
- ‚úÖ **Documented**: Clear examples and usage patterns
- ‚úÖ **Tested**: Comprehensive validation and error handling

### Physics-Focused
- **HEP Datasets**: Designed for typical particle physics data characteristics
- **Signal/Background**: Built-in stratification for classification tasks
- **Time Series**: Handles temporal dependencies in detector data
- **Reproducible**: Consistent random seeds for scientific reproducibility

---

## üìä Current Library Statistics

- **33 Total Extras** across 6 categories
- **16 Core Blocks** in the complete package
- **29 Configuration Files** covering all scenarios  
- **100% Validation Coverage** - all file references verified
- **Zero Duplication** - efficient modular architecture

### Complete Extra Categories:
- üéØ **4 Complete Pipelines**: Ready-to-run end-to-end workflows
- üß† **11 Individual Models**: From decision trees to neural networks
- ‚ö° **9 Algorithm Combos**: Model + preprocessing combinations
- üèóÔ∏è **3 Component Categories**: Preprocessing, feature eng, evaluation
- üìä **5 Data Sources**: Including new splitting functionality
- üåü **1 Special Package**: Comprehensive 'all' bundle

---

## üöÄ Usage Examples

### Quick Start with Data Splitting
```bash
# Install data splitting functionality
mlpipe install-local data-split-validation ./my-hep-analysis

# Use in your Python code
python -c "
from mlpipe.blocks.preprocessing.data_split import split_data
import pandas as pd, numpy as np

# Your HEP data
X = pd.read_csv('my_hep_data.csv') 
y = X.pop('is_signal')

# Split with validation set
splits = split_data(X, y, train_size=0.7, val_size=0.15, test_size=0.15, stratify=True)

# Ready for ML!
X_train, y_train = splits['train']
X_val, y_val = splits['val'] 
X_test, y_test = splits['test']
"
```

### Pipeline Integration
```bash
# List what's available
mlpipe list-extras

# Preview complete setup
mlpipe preview-install model-xgb data-split-validation evaluation

# Install everything needed
mlpipe install-local model-xgb data-split-validation evaluation ./my-project

# Run with custom data splitting
mlpipe run --overrides preprocessing=train_val_test_split model=xgb_classifier
```

---

## üí° Developer Benefits

### Maintainability
- **Helper Functions**: Eliminated code duplication across 33 extras
- **Validation System**: Automatic detection of configuration issues
- **Modular Design**: Easy addition of new models and features

### Extensibility  
- **Standard Patterns**: New models follow established helper function patterns
- **Config-Driven**: New split strategies just need YAML configuration
- **Registry System**: Automatic discovery and registration

### User Experience
- **Self-Documenting**: Clear categorization and detailed descriptions
- **Fail-Fast**: Validation catches issues before installation
- **Flexible**: From simple convenience functions to full pipeline integration

These enhancements transform HEP-ML-Templates into a comprehensive, user-friendly, and production-ready framework for particle physics machine learning, with particular strength in data management and package distribution.
